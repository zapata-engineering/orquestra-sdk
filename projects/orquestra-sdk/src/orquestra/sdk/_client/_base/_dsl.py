################################################################################
# Â© Copyright 2022-2023 Zapata Computing Inc.
################################################################################
from __future__ import annotations

import ast
import inspect
import os
import pathlib
import re
import traceback
import warnings
from collections import OrderedDict
from dataclasses import dataclass
from enum import Enum
from pathlib import Path
from string import Formatter
from tempfile import NamedTemporaryFile
from types import BuiltinFunctionType
from typing import (
    TYPE_CHECKING,
    Any,
    Callable,
    Dict,
    Generic,
    Iterable,
    List,
    NamedTuple,
    Optional,
    Tuple,
    TypeVar,
    Union,
    overload,
)

if TYPE_CHECKING:
    from pip_api._parse_requirements import Requirement

import wrapt  # type: ignore
from orquestra.workflow_shared.exceptions import (
    DirtyGitRepo,
    InvalidTaskDefinitionError,
    WorkflowSyntaxError,
)
from orquestra.workflow_shared.kubernetes.quantity import parse_quantity
from orquestra.workflow_shared.packaging import PackagingError, get_installed_version
from orquestra.workflow_shared.secrets import Secret

# Needed for fully-qualified type annotations.
import orquestra.sdk

from . import _ast

# ----- SDK exceptions  -----


class NoRemoteRepo(Exception):
    pass


class NotGitRepo(Exception):
    pass


class InvalidPlaceholderInCustomTaskNameError(Exception):
    pass


class UnknownPlaceholderInCustomNameWarning(Warning):
    pass


# ----- data structures -----

# Type alias used to mark variables expected to hold raw constant values.
Constant = Any
# Type alias used to mark variables that can be used as task arguments. These are the
# graph nodes that can represent data (contrary to task invocations that represent
# function calls).
Argument = Union[Constant, "ArtifactFuture", "Secret"]

_TaskReturn = TypeVar("_TaskReturn")


@dataclass(frozen=True, eq=True)
class GitImportWithAuth:
    """A task import that uses a private Git repo.

    Please use a helper, such as GithubImport, instead of this class.
    """

    repo_url: str
    git_ref: Union[str, DeferredGitRef]
    username: Optional[str]
    auth_secret: Optional[Secret]
    package_name: Optional[str] = None
    extras: Optional[Tuple[str, ...]] = None


class DeferredGitRef:
    def __init__(self, path_to_repo: Union[str, os.PathLike]):
        self.path_to_repo = path_to_repo

    def resolve(self) -> str:
        """NotAGitRepo: when the local directory is not a valid git repo."""
        import git

        try:
            repo = git.Repo(self.path_to_repo, search_parent_directories=True)
        except (git.InvalidGitRepositoryError, git.NoSuchPathError) as e:
            raise NotGitRepo(f"This is not git repo: {self.path_to_repo}", e)

        return repo.head.object.hexsha


def infer_git_ref(
    local_repo_path: Union[str, os.PathLike] = Path("")
) -> DeferredGitRef:
    """Get git ref to specific local repo.

    The current git repo info is retrieved as default.

    Args:
        local_repo_path: path to the local repo.

    Usage:
    In context of SDK Task imports - infer of the repo will happen at workflow
    submission time to reduce latency.

    @sdk.task(
        source_import=sdk.GithubImport(
            "zapatacomputing/my_source_repo",
            git_ref=sdk.infer_git_ref(),
        )
    )
        def demo_task():
            pass

    Usage for other scenarios:
    def my_fun():
        sdk.infer_git_ref("my/path/").resolve()
    """
    return DeferredGitRef(path_to_repo=local_repo_path)


@dataclass(frozen=True, eq=True)
class GitImport:
    """A task import that uses a Git repository."""

    repo_url: str
    git_ref: Union[str, DeferredGitRef]

    @staticmethod
    def infer(
        local_repo_path: Union[str, os.PathLike] = Path(""),
        git_ref: Optional[str] = None,
    ) -> "DeferredGitImport":
        """Get local Git info for a specific local repo.

        The current git repo info is retrieved as default.

        Args:
            local_repo_path: path to the local repo.
            git_ref: branch/commit/tag.

        Usage:
            ``GitImport.infer()``
        """
        return DeferredGitImport(local_repo_path, git_ref)


def GithubImport(
    repo: str,
    git_ref: str = "main",
    username: Optional[str] = None,
    personal_access_token: Optional[Secret] = None,
    package_name: Optional[str] = None,
    extras: Optional[Union[List[str], str]] = None,
):
    """Helper to create GitImports from Github repos.

    Args:
        repo: The name of the repo from which to import.
        git_ref: the reference to be imported
            (see https://docs.github.com/en/rest/git/refs for details).
        username: the username used to access GitHub
        personal_access_token: must be configured in GitHub for access to the specified
            repo.
        package_name: package name that will be used during pip install. Example:
            my_package @ git+https://github.com/my_repo@main
        extras: name of extra (or list of name of extras) to be installed from repo. Ex:
            my_package[extra] @ git+https://github.com/my_repo@main
            Due to pip restrictions, passing extras require package name to be passed

    Raises:
        TypeError: when a value that is not a `sdk.Secret` is passed as
            `personal_access_token`.
    """
    if personal_access_token is not None and not isinstance(
        personal_access_token, Secret
    ):
        if isinstance(personal_access_token, str):
            errmsg = (
                'You passed a string as `personal_access_token = "..."`. '
                'Please pass `personal_access_token = sdk.Secret(name="...")` instead. '
                "It might seem verbose, but it's a precaution against committing "
                "plain-text credentials to your git repo, or leaking secret values as "
                "part of the workflow definition."
                "\nSuggested fix:\n"
                '  personal_access_token = sdk.Secret(name="paste secret name here")'
            )
        else:
            errmsg = (
                "`personal_access_token` must be of type `sdk.Secret`, "
                f"not {type(personal_access_token).__name__}."
            )
        raise TypeError(errmsg)
    if personal_access_token is not None and isinstance(personal_access_token, Secret):
        if personal_access_token.workspace_id is None:
            warnings.warn(
                "Please specify workspace ID directly for accessing secrets."
                " Support for default workspaces will be sunset in the future.",
                FutureWarning,
            )
    if extras is not None and package_name is None:
        raise TypeError(
            "Due to PIP syntax restrictions, passing extras require" " package name."
        )

    _extras: Optional[Tuple[str, ...]]
    if extras is None:
        _extras = None
    elif isinstance(extras, str):
        _extras = (extras,)
    else:
        _extras = tuple(extras)

    return GitImportWithAuth(
        repo_url=f"https://github.com/{repo}.git",
        git_ref=git_ref,
        username=username,
        auth_secret=personal_access_token,
        extras=_extras,
        package_name=package_name,
    )


class DeferredGitImport:
    def __init__(
        self, local_repo_path: Union[str, os.PathLike], git_ref: Optional[str] = None
    ):
        self.local_repo_path = local_repo_path
        self.git_ref = git_ref

    def resolved(self):
        """Resolve remote URL and git ref based on local git repository.

        Raises:
                NotAGitRepo: when the local directory is not a valid git repo
                NoRemoteRepo: when the remote git repo doesn't exist
                DirtyGitRep: when there is uncommitted change or unpushed commit
                    in a git repo
        """
        import git

        try:
            repo = git.Repo(self.local_repo_path, search_parent_directories=True)
        except (git.InvalidGitRepositoryError, git.NoSuchPathError) as e:
            raise NotGitRepo(f"This is not git repo: {self.local_repo_path}", e)

        # Check if the remote repo exists.
        try:
            remote = repo.remote("origin")
            remote.fetch()
        except (git.GitCommandError, ValueError) as e:
            raise NoRemoteRepo(f"The remote repo {repo} doesn't exist.") from e

        # Check if there is uncommitted change or unpushed commit in a git repo.
        if repo.is_dirty():
            warnings.warn(
                "You have uncommitted changes or unpushed commits in the local repo: "
                f"{repo.working_dir}.",
                DirtyGitRepo,
            )

        repo_url = repo.remotes.origin.url.replace(
            "https://github.com/", "git@github.com:"
        )

        # Check if branch is in the DETACHED HEAD state.
        if self.git_ref is None and repo.head.is_detached:
            warnings.warn("You're working on detached HEAD.")
            self.git_ref = repo.head.object.hexsha
        if self.git_ref is None:
            self.git_ref = repo.head.ref.name

        return GitImport(repo_url=repo_url, git_ref=self.git_ref)


class PythonImports:
    """A task import that uses Python packages."""

    def __init__(
        self,
        # Required packages
        *packages: Union[str, None],
        # Path to a `requirements.txt` file
        file: Optional[str] = None,
    ):
        self._file: Optional[Path]
        if file is not None:
            self._file = pathlib.Path(file)
        else:
            self._file = None
        self._packages = packages

    def resolved(self) -> List[Requirement]:
        from pip_api._parse_requirements import Requirement, parse_requirements

        # on Windows file cannot be reopened when it's opened with delete=True
        # So the temp file is closed first and then deleted manually.
        tmp_file = NamedTemporaryFile(mode="w+", delete=False)
        if self._file is not None:
            # gather all requirements from file
            with open(self._file) as file:
                lines = file.readlines()
            for line in lines:
                tmp_file.write(line)
        # gather all requirements passed by the user
        for package in self._packages:
            tmp_file.write(f"{package}\n")
        tmp_file.flush()
        tmp_file.close()
        # reading all requirements as one parse to avoid conflicts
        requirements = parse_requirements(
            pathlib.Path(tmp_file.name), include_invalid=False
        )
        os.unlink(tmp_file.name)

        # with include_invalid - parsing will never return invalid_requirement type,
        # but mypy doesn't detect that, so this list comp. is to satisfy mypy
        # (and make it type-safe just-in-case
        return [req for req in requirements.values() if isinstance(req, Requirement)]

    def __eq__(self, other):
        if not isinstance(other, PythonImports):
            return False

        return self._file == other._file and self._packages == other._packages

    def __hash__(self):
        return hash((self._file, self._packages))


@dataclass(frozen=True, eq=True)
class LocalImport:
    """Used to specify that the source code is only available locally.

    e.g. not committed to any git repo or in a Python package
    """

    module: str


@dataclass(frozen=True, eq=True)
class InlineImport:
    """A task import that stores the function "inline" with the workflow definition."""

    pass


# If updating this list, you must also update the Import type.
# These are both here to more easily support isinstance(obj, Import)
# Python 3.10 fixes this by allowing Unions in isinstance checks
ImportTypes = (
    LocalImport,
    GitImport,
    GitImportWithAuth,
    DeferredGitImport,
    PythonImports,
    InlineImport,
)
Import = Union[
    LocalImport,
    GitImport,
    GitImportWithAuth,
    DeferredGitImport,
    PythonImports,
    InlineImport,
]
"""Type that includes all possible task imports"""


class Resources(NamedTuple):
    """The computational resources a workflow or task requires.

    If any of these options are omitted, (or is None) the runtime's default value will
    be used.

    If a runtime doesn't support a particular value, it may be silently ignored.
    Please check the documentation for each runtime!

    Args:
        cpu: The requested CPU in "CPU units".
            e.g. for a single CPU core use "1" or "1000m"
        memory: The requested memory in "bytes".
            Binary or decimal suffixes are supported. e.g. "10G" for 10 gigabytes
        disk: The requested disk space in "bytes".
            Binary or decimal suffixes are supported. e.g. "10Gi" for 10 gibibytes
        gpu: Either "0" or "1" to use a GPU or not.
        nodes: The number of nodes requested. This option only applies to workflows.
            This should be a positive integer.
    """

    cpu: Optional[str] = None
    memory: Optional[str] = None
    disk: Optional[str] = None
    gpu: Optional[str] = None
    nodes: Optional[int] = None

    def is_empty(self) -> bool:
        # find out if all the Resources are None
        return all(resource_value is None for resource_value in self._asdict().values())


class DataAggregation(NamedTuple):
    """A class representing information for data aggregation task.

    run - a boolean specifying whether data aggregation should be run (default to True)
    resources - desired resources for data aggregation step (default to:
    cpu=1 memory=2Gi disk =10Gi
    """

    run: Optional[bool] = None
    resources: Resources = Resources()


class ModuleFunctionRef(NamedTuple):
    # Required to dereference function for execution.
    module: str
    function_name: str

    # Needed by Orquestra Studio to power jump-to-definition.
    file_path: Optional[str] = None
    line_number: Optional[int] = None


class FileFunctionRef(NamedTuple):
    # Required to dereference function for execution.
    file_path: str
    function_name: str

    # Needed by Orquestra Studio to power jump-to-definition. `file_path` can
    # be used for both execution and jump-to-definition.
    line_number: Optional[int] = None


class InlineFunctionRef(NamedTuple):
    function_name: str
    fn: Callable


FunctionRef = Union[ModuleFunctionRef, FileFunctionRef, InlineFunctionRef]


class ParameterKind(Enum):
    # Currently this uses the same naming as Python.
    # We only use a subset of Python's kinds, however.
    POSITIONAL_OR_KEYWORD = "POSITIONAL_OR_KEYWORD"
    VAR_POSITIONAL = "VAR_POSITIONAL"
    VAR_KEYWORD = "VAR_KEYWORD"


class TaskParameter(NamedTuple):
    name: str
    kind: ParameterKind


class TaskOutputMetadata(NamedTuple):
    is_subscriptable: bool
    n_outputs: int


def _get_placeholders_from_string(input_string: str) -> List[str]:
    return [i[1] for i in Formatter().parse(input_string) if i[1] is not None]


def parse_custom_name(
    custom_name: Optional[str], signature: inspect.BoundArguments
) -> Optional[str]:
    if custom_name is None:
        return None

    signature.apply_defaults()
    placeholders = _get_placeholders_from_string(custom_name)

    # check if all placeholders are in the arguments of the function
    # We do this by checking all unique placeholders have a corresponding
    # argument in the function's signature
    if diff := set(placeholders) - set(signature.arguments):
        raise InvalidPlaceholderInCustomTaskNameError(
            f"Custom name contains placeholders that" f" aren't parameters: {diff}"
        )
    # replacement string used when value of wildcard is unknown during submit
    replacement_string = "{0}"
    format_dict = {}
    for ph in placeholders:
        if isinstance(signature.arguments[ph], ArtifactFuture):
            fnc = signature.arguments[ph].invocation.task._fn_name
            format_dict[ph] = replacement_string.format(fnc)
            warnings.warn(
                "Custom name contains placeholder with value"
                f" unknown during submission time. Placeholder's value: {ph}"
                f" will be replaced by {replacement_string.format(fnc)}",
                category=UnknownPlaceholderInCustomNameWarning,
            )
        else:
            format_dict[ph] = signature.arguments[ph]
    return custom_name.format(**format_dict)


class TaskDef(wrapt.ObjectProxy, Generic[_TaskReturn]):
    """A function exposed to Orquestra.

    This is the result of applying the @task decorator.

    We do some magic to transform Python code in the workflow function decorated
    function to build the computational workflow graph. Use the `.model` property to
    get the serializable form.
    """

    # The fully-qualified type hint is a workaround for docs' autoapi not being able to
    # resolve symbols.

    def __init__(
        self,
        fn: Callable[..., _TaskReturn],
        output_metadata: "orquestra.sdk._client._base._dsl.TaskOutputMetadata",  # pyright: ignore # NOQA
        source_import: Optional[Import] = None,
        parameters: Optional[OrderedDict] = None,
        dependency_imports: Optional[Tuple[Import, ...]] = None,
        resources: Resources = Resources(),
        custom_image: Optional[str] = None,
        custom_name: Optional[str] = None,
        fn_ref: Optional[FunctionRef] = None,
        max_retries: Optional[int] = None,
        env_vars: Optional[Dict[str, str]] = None,
    ):
        if isinstance(fn, BuiltinFunctionType):
            raise NotImplementedError("Built-in functions are not supported as Tasks")
        super().__init__(fn)
        self.__sdk_task_body = fn
        self._fn_ref = fn_ref
        self._fn_name = fn.__name__
        self._output_metadata = output_metadata
        self._parameters = parameters
        self._resources = resources
        self._custom_image = custom_image
        self._custom_name = custom_name
        self._dependency_imports = dependency_imports
        self._use_default_dependency_imports = dependency_imports is None
        self._source_import = source_import
        self._use_default_source_import = source_import is None
        self._max_retries = max_retries
        self._env_vars = env_vars

        # task itself is not part of any workflow yet. Don't pass wf defaults
        self._resolve_task_source_data()

    @property
    def n_outputs(self):
        warnings.warn(
            '"n_outputs" is deprecated. Please use "output_metadata".',
            DeprecationWarning,
        )
        return self._output_metadata.n_outputs

    def _validate_task_not_in_main(self):
        if (
            not isinstance(self._source_import, InlineImport)
            and isinstance(self._fn_ref, ModuleFunctionRef)
            and self._fn_ref.module == "__main__"
        ):
            err = (
                f"function {self._fn_name} is defined inside __main__ "
                "module. Please move task function to different file and import, "
                "it or mark this task function as inline import \n\n"
                "example: \n"
                "@sdk.task(source_import=sdk.InlineImport())\n"
                f"def {self._fn_name}(): ..."
            )
            raise InvalidTaskDefinitionError(err)

    def _validate_task_resources(self):
        if self._resources:
            if self._resources.cpu is not None:
                cpu = parse_quantity(self._resources.cpu)
                cpu_int = cpu.to_integral_value()
                if cpu_int != cpu and cpu > 1:
                    err = (
                        f"function {self._fn_name} has CPU resource quantity larger "
                        f"then 1, which is not a whole number. Make sure any resource "
                        f">1 is defined as a whole number."
                    )
                    raise InvalidTaskDefinitionError(err)
            if self._resources.gpu is not None:
                try:
                    int_gpu = int(float(self._resources.gpu))
                    float_gpu = float(self._resources.gpu)
                    if int_gpu != float_gpu:
                        raise ValueError
                # Value error can be either if gpus are float, or passed as some form
                # of kubernetes-resource string, like "1000m"
                except ValueError:
                    err = (
                        f"function {self._fn_name} has GPU resource quantity which "
                        f"is not a whole number. Make sure GPU is defined only "
                        f"as integer."
                    )
                    raise InvalidTaskDefinitionError(err)

    def validate_task(self):
        """Validate tasks for possible incompatibilities.

        Raises:
            InvalidTaskDefinitionError: If task is defined in __main__module and is not
                inlined using ``source_import==InlineImport()`` or when an invalid
                number of resources was requested
        """
        try:
            self._validate_task_not_in_main()
            self._validate_task_resources()
        except InvalidTaskDefinitionError:
            raise

    def __call__(
        self, *args: Union[ArtifactFuture, Any], **kwargs: Union[ArtifactFuture, Any]
    ) -> ArtifactFuture[_TaskReturn]:
        try:
            signature = inspect.signature(self.__sdk_task_body).bind(*args, **kwargs)
        except TypeError as exc:
            # Check if an error is generated when the args and kwargs of the task call
            # are bonded to the args and kwargs of the task function.
            summary = traceback.StackSummary.extract(
                traceback.walk_stack(None)
            ).format()[1]
            error_message = (
                f"Error message: {exc}\nThe following assignment could not"
                f" be performed:\n {summary}"
            )
            # Check if the error message informs about not passing a value
            # for the `self` argument. If the `self` argument is missing tells the
            # user to not use methods as task functions.
            missing_self_arg = r".*(missing a required argument:)[^a-zA-Z\d]*(self).*"
            if re.match(missing_self_arg, str(exc)):
                error_message = (
                    f"The task {self._fn_name} seems to be a method, if so"
                    " modify it to not be a method.\n"
                ) + error_message
            raise WorkflowSyntaxError(error_message) from exc

        return ArtifactFuture(
            TaskInvocation(
                self,
                args=args,
                kwargs=tuple(kwargs.items()),
                resources=self._resources,
                custom_name=parse_custom_name(self._custom_name, signature),
                custom_image=self._custom_image,
                env_vars=self._env_vars,
            )
        )

    def _resolve_task_source_data(
        self, wf_default_source_import: Optional[Import] = None
    ):
        # if user set source import explicitly, we use that import
        # else we either take wf default, or base on if the session is interactive
        if self._use_default_source_import:
            if wf_default_source_import:
                self._source_import = wf_default_source_import
            else:
                self._source_import = InlineImport()

        self._resolve_fn_ref()

    def _resolve_fn_ref(self):
        # resolve fn_ref is based on task source import. If user doesn't pass it,
        # resolve_source_import should set it
        assert self._source_import is not None
        self._fn_ref = (
            InlineFunctionRef(self.__sdk_task_body.__name__, self.__sdk_task_body)
            if isinstance(self._source_import, InlineImport)
            else get_fn_ref(self.__sdk_task_body)
        )

    def _resolve_task_dependencies(
        self, wf_default_dependency_imports: Optional[Tuple[Import, ...]] = None
    ):
        # if user set imports explicitly, do nothing
        if not self._use_default_dependency_imports:
            return

        if wf_default_dependency_imports:
            self._dependency_imports = wf_default_dependency_imports


# TaskInvocation is using a Plain Old Python Object on purpose:
# Using POPO instead of a NamedTuple means each instance of TaskInvocation
# is unique, even if they share the exact same attributes. This is required
# for the traversal of the graph.
class TaskInvocation(Generic[_TaskReturn]):
    task: TaskDef

    args: Tuple[Argument, ...]

    # NOTE: Some time ago we needed this to be hashable, hence we went with a tuple of
    # key-value pairs instead of a mapping. It can be changed to a dict.
    kwargs: Tuple[Tuple[str, Argument], ...]

    type: str = "task_invocation"

    # invocation metadata below
    resources: Resources = Resources()
    custom_image: Optional[str] = None

    def __init__(
        self,
        task: TaskDef[_TaskReturn],
        args: Tuple[Argument, ...],
        kwargs: Tuple[Tuple[str, Argument], ...],
        type: str = "task_invocation",
        resources: Resources = Resources(),
        custom_name: Optional[str] = None,
        custom_image: Optional[str] = None,
        env_vars: Optional[Dict[str, str]] = None,
    ):
        self.task = task
        self.args = args
        self.kwargs = kwargs
        self.resources = resources
        self.type = type
        self.custom_name = custom_name
        self.custom_image = custom_image
        self.env_vars = env_vars

    def _asdict(self) -> Dict[str, Any]:
        return {
            "task": self.task,
            "args": self.args,
            "kwargs": self.kwargs,
            "type": self.type,
            "resources": self.resources,
            "custom_name": self.custom_name,
            "custom_image": self.custom_image,
            "env_vars": self.env_vars,
        }


class Sentinel(Enum):
    NO_UPDATE = object()


class ArtifactFormat(Enum):
    # At the moment, we infer artifact serialization format from the object type at run
    # time. See orquestra.sdk._base for details.
    #
    # Values are uppercase here to keep easy interoperability with the workflow json
    # schema.
    # See: orquestra.sdk.schema.ir.ArtifactFormat
    #
    # Add more cases here when we allow users to specify artifact format explicitly.
    # This has to be accompanied by corresponding implementation in orquestra.sdk._base.
    AUTO = "AUTO"


class ArtifactFuture(Generic[_TaskReturn]):
    DEFAULT_CUSTOM_NAME = None
    DEFAULT_SERIALIZATION_FORMAT = ArtifactFormat.AUTO

    def __init__(
        self,
        invocation: TaskInvocation[_TaskReturn],
        output_index: Optional[int] = None,
        custom_name: Optional[str] = DEFAULT_CUSTOM_NAME,
        serialization_format: ArtifactFormat = DEFAULT_SERIALIZATION_FORMAT,
        env_vars: Optional[Dict[str, str]] = None,
    ):
        self.invocation = invocation
        # if the invocation returns multiple values, this the index in the output
        # sequence
        self.output_index = output_index
        # metadata below
        self.custom_name = custom_name
        self.serialization_format = serialization_format
        self.env_vars = env_vars

    def __repr__(self):
        return (
            "ArtifactFuture("
            f"invocation={self.invocation}, "
            f"output_index={self.output_index}, "
            f"custom_name={self.custom_name}, "
            f"serialization_format={self.serialization_format}"
            ")"
        )

    def __getitem__(self, index):
        if not self.invocation.task._output_metadata.is_subscriptable:
            raise TypeError("This ArtifactFuture is not subscriptable")
        if not isinstance(index, int):
            raise TypeError("ArtifactFuture indices must be integers")
        if index >= self.invocation.task._output_metadata.n_outputs:
            raise IndexError("ArtifactFuture index out of range")
        return ArtifactFuture(
            invocation=self.invocation,
            output_index=index,
            custom_name=self.custom_name,
            serialization_format=self.serialization_format,
        )

    def __iter__(self):
        if not self.invocation.task._output_metadata.is_subscriptable:
            raise TypeError("This ArtifactFuture is not iterable")
        futures = [
            ArtifactFuture(
                invocation=self.invocation,
                output_index=next_index,
                custom_name=self.custom_name,
                serialization_format=self.serialization_format,
            )
            for next_index in range(self.invocation.task._output_metadata.n_outputs)
        ]
        return iter(futures)

    def __reduce_ex__(self, protocol):
        """Handles Pickling for ArtifactFuture.

        We currently do not support pickling ArtifactFutures and instead have a
        different format for passing artifacts between tasks.

        The cases where ArtifactFutures can be pickled include when an artifact is
        inside a container (for example `dict()` or `list()`) and we currently do not
        support this pattern.

        Raising an exception here allows us to catch unsupported attempts at pickling
        and let the user know before execution time.
        """
        raise NotImplementedError("ArtifactFuture cannot be pickled")

    # The fully-qualified type hint is a workaround for docs' autoapi not being able to
    # resolve symbols.

    def with_invocation_meta(
        self,
        *,
        cpu: Optional[
            Union[str, "orquestra.sdk._client._base._dsl.Sentinel"]  # pyright: ignore
        ] = Sentinel.NO_UPDATE,
        memory: Optional[
            Union[str, "orquestra.sdk._client._base._dsl.Sentinel"]  # pyright: ignore
        ] = Sentinel.NO_UPDATE,
        disk: Optional[
            Union[str, "orquestra.sdk._client._base._dsl.Sentinel"]  # pyright: ignore
        ] = Sentinel.NO_UPDATE,
        gpu: Optional[
            Union[str, "orquestra.sdk._client._base._dsl.Sentinel"]  # pyright: ignore
        ] = Sentinel.NO_UPDATE,
        custom_image: Optional[
            Union[str, "orquestra.sdk._client._base._dsl.Sentinel"]  # pyright: ignore
        ] = Sentinel.NO_UPDATE,
        env_vars: Optional[
            Union[
                Dict[str, str],
                "orquestra.sdk._client._base._dsl.Sentinel",  # pyright: ignore
            ]
        ] = Sentinel.NO_UPDATE,
    ) -> "ArtifactFuture":
        """
        Assigns optional metadata related to task invocation used to generate this
        artifact.

        Doesn't modify existing invocations, returns a new one.

        Example usage::

            text = capitalize("hello").with_invocation_meta(
                cpu="1000m", custom_image="zapatacomputing/orquestra-qml:v0.1.0-cuda"
            )

        Args:
            cpu: amount of cpu assigned to the task invocation
            memory: amount of memory assigned to the task invocation
            disk: amount of disk assigned to the task invocation
            gpu: amount of gpu assigned to the task invocation
            custom_image: docker image used to run the task invocation
            env_vars: dict of environmental variables to be used in task
        """  # noqa: D205, D212
        self._check_if_destructured(
            fn_name=self.invocation.task._fn_name,
            assign_type="invocation metadata",
        )

        # Only use the new properties if they have not been changed.
        # None is a valid option, so we are using the Sentinel object pattern:
        # https://python-patterns.guide/python/sentinel-object/

        invocation = self.invocation

        resources = invocation.resources
        new_resources = Resources(
            cpu=resources.cpu if cpu is Sentinel.NO_UPDATE else cpu,
            gpu=resources.gpu if gpu is Sentinel.NO_UPDATE else gpu,
            memory=resources.memory if memory is Sentinel.NO_UPDATE else memory,
            disk=resources.disk if disk is Sentinel.NO_UPDATE else disk,
        )

        new_custom_image: Optional[str]

        if custom_image is not Sentinel.NO_UPDATE:
            new_custom_image = custom_image
        else:
            new_custom_image = invocation.custom_image

        if env_vars is not Sentinel.NO_UPDATE:
            new_env_vars = env_vars
        else:
            new_env_vars = invocation.env_vars

        new_invocation = TaskInvocation(
            task=invocation.task,
            args=invocation.args,
            kwargs=invocation.kwargs,
            resources=new_resources,
            custom_name=invocation.custom_name,
            custom_image=new_custom_image,
            env_vars=new_env_vars,
        )

        return ArtifactFuture(
            invocation=new_invocation,
            output_index=self.output_index,
            custom_name=self.custom_name,
            serialization_format=self.serialization_format,
        )

    # The fully-qualified type hint is a workaround for docs' autoapi not being able to
    # resolve symbols.

    def with_resources(
        self,
        *,
        cpu: Optional[
            Union[str, "orquestra.sdk._client._base._dsl.Sentinel"]  # pyright: ignore
        ] = Sentinel.NO_UPDATE,
        memory: Optional[
            Union[str, "orquestra.sdk._client._base._dsl.Sentinel"]  # pyright: ignore
        ] = Sentinel.NO_UPDATE,
        disk: Optional[
            Union[str, "orquestra.sdk._client._base._dsl.Sentinel"]  # pyright: ignore
        ] = Sentinel.NO_UPDATE,
        gpu: Optional[
            Union[str, "orquestra.sdk._client._base._dsl.Sentinel"]  # pyright: ignore
        ] = Sentinel.NO_UPDATE,
    ) -> "ArtifactFuture":
        """
        Assigns optional metadata related to task invocation used to generate this
        artifact.

        Doesn't modify existing invocations, returns a new one.

        Example usage::

            text = capitalize("hello").with_resources(cpu="1000m")

        Args:
            cpu: amount of cpu assigned to the task invocation
            memory: amount of memory assigned to the task invocation
            disk: amount of disk assigned to the task invocation
            gpu: amount of gpu assigned to the task invocation
        """  # noqa: D205, D212
        self._check_if_destructured(
            fn_name=self.invocation.task._fn_name,
            assign_type="resources",
        )

        return self.with_invocation_meta(cpu=cpu, memory=memory, disk=disk, gpu=gpu)

    # The fully-qualified type hint is a workaround for docs' autoapi not being able to
    # resolve symbols.

    def with_custom_image(
        self,
        custom_image: Optional[
            Union[str, "orquestra.sdk._client._base._dsl.Sentinel"]  # pyright: ignore
        ] = Sentinel.NO_UPDATE,
    ) -> "ArtifactFuture":
        """
        Assigns optional metadata related to task invocation used to generate this
        artifact.

        Doesn't modify existing invocations, returns a new one.

        Example usage::

            text = capitalize("hello").with_custom_image(
                "zapatacomputing/orquestra-qml:v0.1.0-cuda"
            )

        Args:
            custom_image: docker image used to run the task invocation
        """  # noqa: D205, D212
        self._check_if_destructured(
            fn_name=self.invocation.task._fn_name,
            assign_type="custom image",
        )

        return self.with_invocation_meta(custom_image=custom_image)

    def with_env_variables(
        self,
        env_vars: Optional[
            Union[
                Dict[str, str],
                "orquestra.sdk._client._base._dsl.Sentinel",  # pyright: ignore
            ]  # pyright: ignore
        ] = Sentinel.NO_UPDATE,
    ) -> "ArtifactFuture":
        """
        Assigns optional metadata related to task invocation used to generate this
        artifact.

        Doesn't modify existing invocations, returns a new one.

        Env vars passed to that function are directly passed to Ray runtime_env object.
        This function overwrites env_vars set by the `env_vars` parameter in the task,
        it does not append them.

        Example usage::

            text = capitalize("hello").with_env_variables(
                {"MY_VAR_NAME": "MY_VAR_VALUE"}
            )

        Args:
            env_vars: dict of all env variables to be set before task starts executing
        """  # noqa: D205, D212
        self._check_if_destructured(
            fn_name=self.invocation.task._fn_name,
            assign_type="env_vars",
        )

        return self.with_invocation_meta(env_vars=env_vars)

    def _check_if_destructured(self, fn_name: str, assign_type: str):
        """Check if an ArtifactFuture has been destructured.

        The fn_name and assign_type are used to construct an informative error message
        if the ArtifactFuture is destructured.

        Args:
            fn_name: The name of the function that created this ArtifactFuture.
            assign_type: The type of metadata being added to the ArtifactFuture.

        Raises:
            WorkflowSyntaxError: if the ArtifactFuture has been destructured
        """
        if self.output_index is not None:
            summary = traceback.StackSummary.extract(
                traceback.walk_stack(None)
            ).format()[2]
            if assign_type == "custom image":
                assign_type = "a " + assign_type
            raise WorkflowSyntaxError(
                f"Can't assign {assign_type} to an artifact that"
                " has been destructured.\n"
                f"To assign {assign_type} to a call of the task "
                f"{fn_name}"
                " make sure NOT to destructure its outputs. "
                f"The following assignment could not be performed:\n {summary}"
            )


# ----- decorators -----


def _resolve_module_path(fn):
    if (abs_path := inspect.getsourcefile(fn)) is None:
        # This can happen when the `fn` is defined in a binary module. We need to check
        # it explicitly to appease mypy. It would be nice to have a test case for it.
        return None
    try:
        relative_path = os.path.relpath(abs_path)
    except ValueError:
        relative_path = os.path.abspath(abs_path)
    return pathlib.Path(relative_path).as_posix()


def _get_fn_line_number(fn):
    line_number: Optional[int]
    try:
        line_number = inspect.getsourcelines(fn)[1]
    except OSError:
        line_number = None
    return line_number


def get_fn_ref(fn) -> FunctionRef:
    return ModuleFunctionRef(
        module=fn.__module__,
        function_name=fn.__name__,
        file_path=_resolve_module_path(fn),
        line_number=_get_fn_line_number(fn),
    )


def _parameter_kind_from_inspect(kind: inspect._ParameterKind) -> ParameterKind:
    if kind == inspect._ParameterKind.VAR_POSITIONAL:
        return ParameterKind.VAR_POSITIONAL
    elif kind == inspect._ParameterKind.VAR_KEYWORD:
        return ParameterKind.VAR_KEYWORD
    else:
        return ParameterKind.POSITIONAL_OR_KEYWORD


def _get_parameters(fn: Callable) -> OrderedDict:
    sig = inspect.signature(fn)
    # A dict is used in order to easily reference Parameters by name
    # However, parameters are strictly ordered: An OrderedDict must be used
    parameters = OrderedDict()
    for param in sig.parameters.values():
        task_parameter = TaskParameter(
            name=param.name, kind=_parameter_kind_from_inspect(param.kind)
        )
        parameters[param.name] = task_parameter
    return parameters


def _get_number_of_outputs(fn: Callable) -> TaskOutputMetadata:
    try:
        source = inspect.getsource(fn)
    except OSError:
        # Unable to find source, assume 1 output
        return TaskOutputMetadata(is_subscriptable=False, n_outputs=1)
    fn_body = ast.parse(_ast.normalize_indents(source))
    visitor = _ast.OutputCounterVisitor()
    visitor.visit(fn_body)

    n_different_returns = len(visitor.outputs)
    if n_different_returns == 0:
        # No outputs detected - probably a void function with not return statement
        return TaskOutputMetadata(is_subscriptable=False, n_outputs=1)
    elif n_different_returns == 1:
        (ast_outputs,) = visitor.outputs
        return TaskOutputMetadata(
            is_subscriptable=ast_outputs.is_subscriptable,
            n_outputs=ast_outputs.n_outputs,
        )
    else:
        warnings.warn(
            "Complex function detected, falling back to a single output. "
            f"Hypotheses: {visitor.outputs}"
        )
        return TaskOutputMetadata(is_subscriptable=False, n_outputs=1)


@overload
def task(fn: Callable[..., _TaskReturn]) -> TaskDef[_TaskReturn]:
    ...


@overload
def task(
    *,
    source_import: Optional[Import] = None,
    dependency_imports: Union[Iterable[Import], Import, None] = None,
    resources: Resources = Resources(),
    n_outputs: Optional[int] = None,
    custom_image: Optional[str] = None,
    custom_name: Optional[str] = None,
    max_retries: Optional[int] = None,
    env_vars: Optional[Dict[str, str]] = None,
) -> Callable[[Callable[..., _TaskReturn]], TaskDef[_TaskReturn]]:
    ...


@overload
def task(
    fn: Callable[..., _TaskReturn],
    *,
    source_import: Optional[Import] = None,
    dependency_imports: Union[Iterable[Import], Import, None] = None,
    resources: Resources = Resources(),
    n_outputs: Optional[int] = None,
    custom_image: Optional[str] = None,
    custom_name: Optional[str] = None,
    max_retries: Optional[int] = None,
    env_vars: Optional[Dict[str, str]] = None,
) -> TaskDef[_TaskReturn]:
    ...


def task(
    fn: Optional[Callable[..., _TaskReturn]] = None,
    *,
    source_import: Optional[Import] = None,
    dependency_imports: Union[Iterable[Import], Import, None] = None,
    resources: Resources = Resources(),
    n_outputs: Optional[int] = None,
    custom_image: Optional[str] = None,
    custom_name: Optional[str] = None,
    max_retries: Optional[int] = None,
    env_vars: Optional[Dict[str, str]] = None,
) -> Union[
    TaskDef[_TaskReturn], Callable[[Callable[..., _TaskReturn]], TaskDef[_TaskReturn]]
]:
    """Wraps a function into an Orquestra Task.

    The result is something you can use inside your `@sdk.workflow` function. If you
    need to call the task's underlying function directly, see
    ``orquestra.sdk.packaging.execute_task()``.

    Args:
        fn: A function definition to expose to Orquestra.
        source_import: Tells Orquestra what "import" needs to be installed to access
            this task's code. For more information see the ``Import``'s union definition
            and the guide:
            https://docs.orquestra.io/docs/core/sdk/guides/dependency-installation.html
        dependency_imports: Tells Orquestra what other "imports" need to be
            installed before running this task. For more information see:
            https://docs.orquestra.io/docs/core/sdk/guides/dependency-installation.html
        resources: hints Orquestra what computational resources are required to
            run this task. For more information see:
            https://docs.orquestra.io/docs/core/sdk/guides/resource-management.html
        n_outputs: tells Orquestra how many outputs this task produces. If omitted,
            the SDK magically infers this information from the task function's source
            code by analyzing the Abstract Syntax Tree (AST).
        custom_image: tell the runtime to run this task in a docker container
            preloaded with a custom docker image. Only supported with remote workflows
            sent to Compute Engine.
        custom_name: changes name for invocation of this task. Supports python
            formatting in brackets {} using task parameters. Currently, supports only
            values known at submit time. If parameter is unknown at submit time (e.g.
            result of other task) - it will be placeholded. Every character that is
            non-alphanumeric will be changed to dash ("-").
            Also only first 128 characters of the name will be used
        max_retries: Maximum number of times a worker will try to retry after failure.
            Useful if worker is killed by random events, or memory leaks from previously
            executed tasks.
            WARNING: retried workers might cause issues in MLflow logging, as retried
            workers share the same invocation ID, MLflow identifier will be shared
            between them.
        env_vars: environmental variables that will be set on a worker before
            the task is scheduled
    Raises:
        ValueError: when a task has fewer than 1 outputs.
    """
    task_dependency_imports: Optional[Tuple[Import, ...]]

    if dependency_imports is None:
        task_dependency_imports = None
    elif isinstance(dependency_imports, ImportTypes):
        task_dependency_imports = (dependency_imports,)
    elif dependency_imports is not None:
        task_dependency_imports = tuple(dependency_imports)

    if n_outputs is not None:
        if n_outputs <= 0:
            raise ValueError("A task should have at least one output")

    def _inner(fn: Callable):
        # Assume if a user has specified the number of outputs, then this output is
        # subscriptable
        output_metadata: TaskOutputMetadata
        if n_outputs is not None:
            output_metadata = TaskOutputMetadata(
                is_subscriptable=True, n_outputs=n_outputs
            )
        else:
            output_metadata = _get_number_of_outputs(fn)

        task_def = TaskDef(
            fn=fn,
            source_import=source_import,
            dependency_imports=task_dependency_imports,
            resources=resources,
            parameters=_get_parameters(fn),
            output_metadata=output_metadata,
            custom_image=custom_image,
            custom_name=custom_name,
            max_retries=max_retries,
            env_vars=env_vars,
        )

        return task_def

    if fn is None:
        return _inner
    else:
        return _inner(fn)


def InstalledImport(
    *,
    package_name: str,
    version_match: Optional[str] = None,
    fallback: Optional[Import] = None,
) -> Import:
    """Returns PythonImports for a task for the installed version of a package.

    On an error, if a fallback is provided, the fallback is returned instead.

    Args:
        package_name: the package to use as the import.
        version_match: a regex string to match the installed version.
        fallback: a fallback import to return if there are any issues
            in finding the package version.

    Raises:
        PackagingError: If there are any issues finding an installed package and no
            fallback is provided.

    Returns:
        Either a PythonImports for package_name at the installed version or `fallback`.

    """
    try:
        version = get_installed_version(package_name)
        if version_match is not None:
            if re.match(version_match, version) is None:
                raise PackagingError(
                    f"Package version mismatch: "
                    f"{package_name}=={version}\n"
                    f'Expected version to match "{version_match}"'
                )
    except PackagingError as e:
        if fallback is not None:
            return fallback
        raise e

    return PythonImports(f"{package_name}=={version}")


def execute_task(task: TaskDef, args: tuple, kwargs: dict):
    """A helper method to testing tasks by executing them outside of the workflow graph.

    Use only for unittesting code.

    Args:
        task: the task you want to execute.
        args: the positional arguments to pass to the task.
        kwargs: the keyword arguments to pass to the task.

    Returns:
        the result of executing a task.
    """
    return task._TaskDef__sdk_task_body(*args, **kwargs)
